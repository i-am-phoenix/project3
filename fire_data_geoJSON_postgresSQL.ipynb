{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine\n",
    "from flask import Flask, jsonify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in final data from csv\n",
    "fire_data = pd.read_csv(\"fire_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1376, 27)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check df dimensions\n",
    "fire_data.shape\n",
    "\n",
    "# # checking column types\n",
    "# for col in fire_data.columns:\n",
    "#     print(col, type(fire_data[col][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1376, 27)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert table column names to lowercase\n",
    "fire_data.columns = fire_data.columns.str.lower()\n",
    "fire_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1023, 27)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some fires are spread through multiple counties, concatenate the county names and types to avoid duplicate entries\n",
    "fire_data['county'] = fire_data.groupby(['firename'])['county'].transform(lambda x : ', '.join(x))\n",
    "fire_data['caucus'] = fire_data.groupby(['firename'])['caucus'].transform(lambda x : ', '.join(x))  \n",
    "\n",
    "# drop duplicate data\n",
    "fire_data = fire_data.drop_duplicates(subset=['firename'])   \n",
    "  \n",
    "# show the dataframe\n",
    "fire_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating geoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to convert DF into geoJSON format\n",
    "def df_to_geojson(df, properties, lat='latitude', lon='longitude'):\n",
    "    # create a new python dict to contain our geojson data, using geojson format\n",
    "    geojson = {'type':'FeatureCollection', 'features':[]}\n",
    "\n",
    "    # loop through each row in the dataframe and convert each row to geojson format\n",
    "    for _, row in df.iterrows():\n",
    "        # create a feature template to fill in\n",
    "        feature = {'type':'Feature',\n",
    "                   'properties':{},\n",
    "                   'geometry':{'type':'Point',\n",
    "                               'coordinates':[]}}\n",
    "\n",
    "        # fill in the coordinates\n",
    "        feature['geometry']['coordinates'] = [row[lon],row[lat]]\n",
    "\n",
    "        # for each column, get the value and add it as a new feature property\n",
    "        for prop in properties:\n",
    "            if type(row[prop]) != str:\n",
    "                feature['properties'][prop] = str(row[prop])\n",
    "            else :\n",
    "                feature['properties'][prop] = row[prop]\n",
    "        \n",
    "        # add this feature (aka, converted dataframe row) to the list of features inside our dict\n",
    "        geojson['features'].append(feature)\n",
    "    \n",
    "    return geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List properties to be passed to geoJSON\n",
    "# cols = ['acresburned', 'adminunit', 'archiveyear', 'county', 'extinguished',\n",
    "#        'fatalities', 'firelocation', 'firename',\n",
    "#        'searchdescription', 'started', 'structuresdamaged',\n",
    "#        'structuresdestroyed', 'structuresevacuated', 'structuresthreatened',\n",
    "#        'timestarted', 'timeextinguished', 'dayofweekstartedname',\n",
    "#        'dayofweekstartednum', 'duration', 'date_established',\n",
    "#        'population_jul_2019', 'areasqmi', 'areakm2', 'popdensitypersqmi',\n",
    "#        'caucus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1023"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_geoJSON = df_to_geojson(fire_data , fire_data.drop(['latitude','longitude'], axis=1).columns)\n",
    "\n",
    "# Write geoJSON formatted text to a text file\n",
    "with open(\"fire_data.json\", \"w\") as output:\n",
    "    json.dump(fire_geoJSON, output)\n",
    "    \n",
    "# review geoJSON\n",
    "len(fire_geoJSON[\"features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushing data to postgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup connection\n",
    "rds_connection_string = \"postgres:postgres@localhost:5432/ca_fire\"\n",
    "engine = create_engine(f'postgresql://{rds_connection_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fire_data'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  review table names\n",
    "table_name = engine.table_names()[0]\n",
    "table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typecast each datetime column to appropriate date type for SQL import\n",
    "fire_data.started=pd.to_datetime(fire_data.started)\n",
    "fire_data.extinguished=pd.to_datetime(fire_data.extinguished)\n",
    "fire_data.timestarted=pd.to_datetime(fire_data.timestarted)\n",
    "fire_data.timeextinguished=pd.to_datetime(fire_data.timeextinguished)\n",
    "\n",
    "# # checking column types\n",
    "# for col in fire_data.columns:\n",
    "#     print(col, type(fire_data[col][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_data.to_csv(\"fire_data_1.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to postgreSQL database\n",
    "fire_data.to_sql(name=table_name, con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sourcing data from postgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLask path 1: Pull data for top 10 longest burning fires in each year\n",
    "\n",
    "query_top_fires_duration = \"\"\"\n",
    "SELECT firename, firelocation, archiveyear, started, extinguished, latitude, longitude, \n",
    "       acresburned, fatalities, dayofweekstartedname, dayofweekstartednum,\n",
    "       duration, county, caucus, year_rank \n",
    "    FROM (\n",
    "\tSELECT\n",
    "\t\t*, \n",
    "\t\tRANK () OVER ( \n",
    "\t\t\tPARTITION BY p.archiveyear\n",
    "\t\t\tORDER BY duration DESC\n",
    "\t\t) year_rank \n",
    "\tFROM\n",
    "\t\tfire_data p)\n",
    "AS x WHERE year_rank < 11\n",
    "\"\"\"\n",
    "# Execute sql query \n",
    "data_top_fires_duration = engine.execute(query_top_fires_duration)  \n",
    "\n",
    "# Pull data table column names\n",
    "table_headers = engine.execute(query_top_fires_duration)._metadata.keys\n",
    "\n",
    "# convert to DF\n",
    "df_top_fires_duration = pd.DataFrame(data_top_fires_duration, columns=table_headers)\n",
    "# df_top_fires_duration.head()\n",
    "\n",
    "# Convert refined data frame to geoJSON format\n",
    "top_fires_duration_geoJSON = df_to_geojson(\n",
    "    df_top_fires_duration, \n",
    "    df_top_fires_duration.drop(['latitude','longitude'], axis=1).columns)\n",
    "# top_fires_duration_geoJSON\n",
    "\n",
    "# Write geoJSON formatted text to a text file\n",
    "with open(\"top_fires_duration.json\", \"w\") as output:\n",
    "    json.dump(top_fires_duration_geoJSON, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_fires_duration_geoJSON['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLask path 2: Pull data for top 10 largest (acres affected) fires in each year\n",
    "\n",
    "query_top_fires_acres = \"\"\"\n",
    "SELECT firename, firelocation, archiveyear, started, extinguished, latitude, longitude, \n",
    "       acresburned, fatalities, dayofweekstartedname, dayofweekstartednum,\n",
    "       duration, county, caucus, year_rank \n",
    "    FROM (\n",
    "\tSELECT\n",
    "\t\t*, \n",
    "\t\tRANK () OVER ( \n",
    "\t\t\tPARTITION BY p.archiveyear\n",
    "\t\t\tORDER BY acresburned DESC\n",
    "\t\t) year_rank \n",
    "\tFROM\n",
    "\t\tfire_data p)\n",
    "AS x WHERE year_rank < 11\n",
    "\"\"\"\n",
    "# Execute sql query \n",
    "data_top_fires_acres = engine.execute(query_top_fires_acres)  \n",
    "\n",
    "# Pull data table column names\n",
    "table_headers = engine.execute(query_top_fires_acres)._metadata.keys\n",
    "\n",
    "# convert to DF\n",
    "df_top_fires_acres = pd.DataFrame(data_top_fires_acres, columns=table_headers)\n",
    "# df_top_fires_acres.head()\n",
    "\n",
    "# Convert refined data frame to geoJSON format\n",
    "top_fires_acres_geoJSON = df_to_geojson(df_top_fires_acres , df_top_fires_acres.drop(['latitude','longitude'], axis=1).columns)\n",
    "# top_fires_acres_geoJSON\n",
    "\n",
    "# Write geoJSON formatted text to a text file\n",
    "with open(\"top_fires_acres.json\", \"w\") as output:\n",
    "    json.dump(top_fires_acres_geoJSON, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLask path 3: Pull data for top 10 deadliest fires overall\n",
    "\n",
    "query_deadliest = \"\"\"\n",
    "SELECT firename, firelocation, archiveyear, started, extinguished, latitude, longitude, \n",
    "       acresburned, fatalities, dayofweekstartedname, dayofweekstartednum,\n",
    "       duration, county, caucus\n",
    "FROM fire_data WHERE fatalities > 0 ORDER BY fatalities DESC LIMIT 10\n",
    "\"\"\"\n",
    "# Execute sql query \n",
    "data_deadliest_fires = engine.execute(query_deadliest)  \n",
    "\n",
    "# Pull data table column names\n",
    "table_headers = engine.execute(query_deadliest)._metadata.keys\n",
    "\n",
    "# convert to DF\n",
    "df_deadliest = pd.DataFrame(data_deadliest_fires, columns=table_headers)\n",
    "# df_deadliest\n",
    "\n",
    "# Convert refined data frame to geoJSON format\n",
    "daedliest_fires_geoJSON = df_to_geojson(df_deadliest , df_deadliest.drop(['latitude','longitude'], axis=1).columns)\n",
    "# daedliest_fires_geoJSON\n",
    "\n",
    "# Write geoJSON formatted text to a text file\n",
    "with open(\"deadliest_fires.json\", \"w\") as output:\n",
    "    json.dump(daedliest_fires_geoJSON, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLask path 4: Pull data for all fires\n",
    "query_all = \"\"\"SELECT * FROM fire_data;\"\"\"\n",
    "# Execute sql query \n",
    "data_all_fires = engine.execute(query_all)  \n",
    "\n",
    "# Pull data table column names\n",
    "table_headers = engine.execute(query_all)._metadata.keys\n",
    "\n",
    "# convert to DF\n",
    "df_all_fires = pd.DataFrame(data_all_fires, columns=table_headers)\n",
    "# df_deadliest\n",
    "\n",
    "# Convert refined data frame to geoJSON format\n",
    "all_fires_geoJSON = df_to_geojson(df_all_fires , df_all_fires.drop(['latitude','longitude'], axis=1).columns)\n",
    "# daedliest_fires_geoJSON\n",
    "\n",
    "# Write geoJSON formatted text to a text file\n",
    "with open(\"all_fires.json\", \"w\") as output:\n",
    "    json.dump(all_fires_geoJSON, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
